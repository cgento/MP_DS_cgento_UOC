\chapter{Panel Attrition: Causas, predicción y machine learning}
\label{chapter:attrition}
\section{Causas del panel attrition, tratamiento y métodos adaptativos y reactivos}

Conceptualmente, las causas del panel attrition pueden clasificarse en tres categorías secuenciales (\cite{lepkowski2002nonresponse}): no-localización, no-contacto y no-cooperación.

La no-localización se refiere a no conseguir localizar a los hogares panelistas durante el período de campo. Esto suele ocurrir por cambios o errores en la información de contacto recogida durante la ola anterior (direcciones de residencia, teléfonos o email) (\cite{couper2009keeping}). Algunos factores que pueden contribuir al éxito o fracaso en la localización son el método de recolección de datos, la propensión a los cambios de localización de los encuestados entre diferentes olas, el tiempo transcurrido entre olas o el presupuesto (\cite{lynn2009methods}).

La segunda causa es el no-contacto. Se trata de, tras localizar exitosamente al panelista, no conseguir establecer un contacto. Para que haya contacto exitoso, es necesario que el intento de contacto por parte de los encuestadores coincida temporalmente con la disponibilidad del panelista. Esto depende completamente del método de recolección de los datos. Por ejemplo, en una entrevista personal, el panelista debe estar en su residencia justo en el mismo momento en el que el entrevistador hace la visita al hogar.

Finalmente, después de establecer el contacto, es necesario convencer al panelista para que vuelva a participar otra vez en la encuesta. La falta de cooperación es una preocupación común en todo tipo de encuestas, y suelen destacar factores como las características socio-demográficas de los encuestados o la temática de la encuesta (\cite{groves1992understanding}). En el caso particular de las encuestas longitudinales, los encuestados además tienen experiencia previa por haber participado en ediciones pasadas, y esto puede potenciar el efecto de factores como la duración de la entrevista, la carga cognitiva de pensar las respuestas o la fatiga por haber participado en ediciones anteriores (\cite{laurie1999strategies}, \cite{watson2009identifying}, \cite{lynn2018tackling}).

Con respecto a las consecuencias del Panel Attriton, \cite{lynn2018tackling} destaca dos principales problemas. Por un lado, si la tasa de attrition es alta, el tamaño de la muestra se reducirá drásticamente con el paso de las olas, lo que provocará que la precisión de los estimadores de la encuesta sea muy baja, y además limitará o incluso imposibilitará el análisis de subgrupos dentro de la muestra. Por otro lado, si el Panel Attrition no es aleatorio, y por tanto los panelistas que abandonan la encuesta son sistemáticamente diferentes a los que se mantienen, existe el riesgo de introducir un sesgo de no-respuesta en los estimadores.

Tradicionalmente, los efectos del Panel Attrition se han mitigado con la implementación de métodos de imputación múltiple (\cite{rubin1987multiple}), reponderando los pesos muestrales (\cite{groves2009survey}) e introduciendo muestras de refresco para sustituir a las unidades muestrales perdidas (\cite{hirano1998combining}). Pero en las últimas décadas se ha extendido el uso de los llamados diseños adaptativos y reactivos (adaptative and responsive designs, \cite{groves2006responsive}, \cite{wagner2008adaptive}, \cite{schouten2017adaptive}, \cite{tourangeau2017adaptive}). Estos diseños se fundamentan en utilizar toda la información que se genera durante la elaboración de encuestas (respuestas al cuestionario, paradata, observaciones de los entrevistadores, información de olas pasadas...) para diseñar implementaciones informadas cuyo objetivo sea mejorar la calidad de los datos, reducir los costes, o ambos. Por ejemplo, se han utilizado para revisar incentivos a participar para grupos concretos de encuestados \cite{mcgonagle2022effects}, revisar el orden de las preguntas (\cite{early2017dynamic} o utilizar los registros de llamadas a panelistas en ediciones anteriores para optimizar las estrategias de contacto en las ediciones siguientes (\cite{kreuter2015note}).

Dentro de este contexto de diseños adaptativos y reactivos, en las últimas décadas destaca el desarrollo del uso de algoritmos de machine learning en la metodología de encuestas, y en particular para predecir no-respuesta y panel attrition (\cite{buskirk2018introduction}, \cite{kern2019tree}). Se comenta con más detalle en el siguiente apartado.

\section{Predicción de Panel Attrition con machine learning}

Los modelos utilizados en metodología de encuestas pueden clasificarse en dos categorías según sus objetivos: modelos para explicar, y modelos para predecir. Los modelos explicativos utilizan toda la información disponible para explorar las relaciones entre diferentes variables observadas, identificar causalidad entre ellas y realizar ejercicios de inferencia y contrastes de hipótesis. Los modelos predictivos, en cambio, buscan predecir o clasificar con precisión el valor de ciertas variables para escenarios que todavía no han ocurrido. Por construcción, sólo utilizan la información disponible antes de que ocurra el suceso a predecir. Aunque los modelos basados en métodos de machine learning pueden ser utilizados para ambas tareas, son particularmente interesantes para realizar tareas de predicción. En \cite{buskirk2018introduction} destacan particularmente la flexibilidad de los modelos de machine learning con respecto a otros modelos tradicionales utilizados en la metodología de encuestas, como la regresión logística o los mínimos cuadrados ordinarios (OLS). Para muchos algoritmos de machine learning no es necesario hacer supuestos sobre las distribuciones de las variables, hacer una especificación explícita de las relaciones entre variables antes de estimar los modelos, y además soportan el uso de un gran número de variables. Esto les permite detectar patrones y relaciones complejas entre variables y los convierte en una herramienta muy útil para realizar tareas de predicción.

En el contexto de la predicción del panel attrition, muchos estudios comparan el rendimiento de modelos de machine learning con el de modelos que se han utilizado tradicionalmente para analizar panel attrition, generalmente una regresión logística o Logit de efectos principales, es decir, sin considerar interacciones entre variables ni relaciones no linales. En \cite{kern2019tree} y \cite{kern2021predicting} utilizan datos de dos paneles de hogares en alemania para comparar el rendimiento de un Logit con varios modelos basados en árboles de decisión. Los resultados son prometedores ya que todos los modelos basados en árboles mostraron mejores rendimientos que el Logit, y destacan por su facilidad para ser interpretados. Sin embargo, estos resultados no parecen ser extrapolables a todo tipo de encuestas. Otro ejemplo prometedor puede verse es en \cite{beste2023case}. Este estudio se divide en dos partes. En primer lugar, se utiliza información de ediciones pasadas de una encuesta a hogares en alemania para comparar, de nuevo, el rendimiento de un Logit con un algoritmo k-nearest neighbours (kNN), un árbol de clasificación (CART), un Random Forest (RF) y un Gradient Boosting Machine (GBM). El modelo que ofreció mejores resultados fue el Random Forest. Y en la segunda parte, utilizaron ese modelo de Random Forest para identificar hogares panelistas con una baja propensión a participar en la edición siguiente de la encuesta. Esa información les sirvió para crear un diseño experimental en el cual se asignaba un incentivo monetario adicional a la mitad de ésos hogares. Sus resultados del experimento mostraron incrementos en las tasas de respuesta de los hogares tratados, y animaron a sus responsables a seguir utilizando este diseño adaptativo en futuras ediciones.

Aunque sin duda estos resultados son prometedores, es importante recalcar que deben ser contextualizados y valorados para cada caso de análisis. Por ejemplo, en \cite{liu2020using} se utiliza un panel de individuos en Estados Unidos para predecir la participación de panelistas en la segunda edición de dicha encuesta, y se compara de nuevo el rendimiento de un Logit con los de un Random Forest (RF), un modelo de Máquinas de Soporte Vectorial (SVM) y un LASSO. Sólo el LASSO mostró una mejora con respecto al modelo de regresión logística. Otro ejemplo puede verse en \cite{jankowsky2022validation}. En este estudio se compara el rendimiento de un modelo Logit con el de un modelo GBM (Gradient Boosting Machine) en dos encuestas con diseños bastante diferentes (una encuesta es en EEUU y la otra en alemania, una se ha realizado cada nueve años y la otra anualmente, una es telefónica y la otra es una entrevista presencial...). Para ambas encuestas apenas se observa que el GBM mejore los resultados de la regresión logística.

Como resumen, podemos decir que los métodos de machine
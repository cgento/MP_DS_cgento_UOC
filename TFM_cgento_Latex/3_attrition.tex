\chapter{Panel Attrition: Causas, predicción y machine learning}
\label{chapter:attrition}
\section{Causas del panel attrition, tratamiento y métodos adaptativos}

Conceptualmente, las causas del panel attrition pueden clasificarse en tres categorías secuenciales (\cite{lepkowski2002nonresponse}): no-localización, no-contacto y no-cooperación.

La no-localización se refiere a no conseguir localizar a los hogares panelistas durante el período de campo. Esto suele ocurrir por cambios o errores en la información de contacto recogida durante la ola anterior (direcciones de residencia, teléfonos o email) (\cite{couper2009keeping}). Algunos factores que pueden contribuir al éxito o fracaso en la localización son el método de recolección de datos, la propensión a los cambios de localización de los encuestados entre diferentes olas, el tiempo transcurrido entre olas o el presupuesto (\cite{lynn2009methods}).

La segunda causa es el no-contacto. Se trata de, tras localizar exitosamente al panelista, no conseguir establecer un contacto. Para que haya contacto exitoso, es necesario que el intento de contacto por parte de los encuestadores coincida temporalmente con la disponibilidad del panelista. Esto depende completamente del método de recolección de los datos. Por ejemplo, en una entrevista personal, el panelista debe estar en su residencia justo en el mismo momento en el que el entrevistador hace la visita al hogar.

Finalmente, después de establecer el contacto, es necesario convencer al panelista para que vuelva a participar otra vez en la encuesta. La falta de cooperación es una preocupación común en todo tipo de encuestas, y suelen destacar factores como las características socio-demográficas de los encuestados o la temática de la encuesta (\cite{groves1992understanding}). En el caso particular de las encuestas longitudinales, los encuestados además tienen experiencia previa por haber participado en ediciones pasadas, y esto puede potenciar el efecto de factores como la duración de la entrevista, la carga cognitiva de pensar las respuestas o la fatiga por haber participado en ediciones anteriores (\cite{laurie1999strategies}, \cite{watson2009identifying}, \cite{lynn2018tackling}).

Con respecto a las consecuencias del Panel Attriton, \cite{lynn2018tackling} destaca dos principales problemas. Por un lado, si la tasa de attrition es alta, el tamaño de la muestra se reducirá drásticamente con el paso de las olas, lo que provocará que la precisión de los estimadores de la encuesta sea muy baja, y además limitará o incluso imposibilitará el análisis de subgrupos dentro de la muestra. Por otro lado, si el Panel Attrition no es aleatorio, y por tanto los panelistas que abandonan la encuesta son sistemáticamente diferentes a los que se mantienen, existe el riesgo de introducir un sesgo de no-respuesta en los estimadores.

Tradicionalmente, los efectos del Panel Attrition se han mitigado con la implementación de métodos de imputación múltiple (\cite{rubin1987multiple}), reponderando los pesos muestrales (\cite{groves2009survey}) e introduciendo muestras de refresco para sustituir a las unidades muestrales perdidas (\cite{hirano1998combining}). Pero en las últimas décadas se ha extendido el uso de los llamados diseños adaptativos y reactivos (adaptative and responsive designs, \cite{groves2006responsive}, \cite{wagner2008adaptive}, \cite{schouten2017adaptive}, \cite{tourangeau2017adaptive}). Estos diseños se fundamentan en utilizar toda la información que se genera durante la elaboración de encuestas (respuestas al cuestionario, paradata u observaciones de los entrevistadores) para diseñar implementaciones informadas cuyo objetivo sea mejorar la calidad de los datos, reducir los costes, o ambos. Por ejemplo, se han utilizado para revisar incentivos a participar para grupos concretos de encuestados \cite{mcgonagle2022effects}, revisar el orden de las preguntas (\cite{early2017dynamic}.

Enfocándonos en el contexto específico de las encuestas longitudinales, éstas ofrecen grandes oportunidades para implementar este tipo de diseños porque recopilan información tanto de los trabajos de campo actuales, como de todos los procesos realizados durante olas anteriores. Por ejemplo, en \cite{kreuter2015note} utilizan los registros de llamadas a panelistas en ediciones anteriores para optimizar las estrategias de contacto en las ediciones siguientes.

\section{Predicción de Panel Attrition con machine learning}

La extensión de los diseños adaptativos y reactivos ha favorecido que en las últimas décadas se haya desarrollado el uso algoritmos de Machine Learning en la metodología de encuestas. En \cite{buskirk2018introduction} destacan que los algoritmos de Machine Learning son una extensión con respecto a métodos tradicionales usados en metodología de encuestas, como la regresión logística y la estimación por mínimos cuadrados ordinarios (OLS). Destacan particularmente su flexibilidad, ya que para muchos algoritmos de machine learning no es necesario hacer supuestos sobre las distribuciones de las variables, hacer una especificación explícita de los modelos antes de estimarlos, y además permiten el uso de un gran número de variables.



as técnicas de machine learning suponen una ampliación con respecto a métodos tradicionales como la regresión logística o los mínimos cuadrados ordinarios (OLS), y  de los métodos utilizados tradicionalmente por los investigadores, \cite{kern2019tree}).De manera particular, han presentado resultados prometedores a la hora de predecir resultados de participación en los trabajos de campo, especialmente los modelos basados en árboles de decisión (\cite{kern2019tree}, \cite{kern2021predicting}, \cite{liu2020using}). En el contexto de Panel Attrition, en \cite{beste2023case} utilizaron información de ediciones pasadas de una encuesta a hogares en alemania para entrenar un Random Forest e identificar hogares panelistas con una baja propensión a participar. Luego, utilizaron esa información para crear un diseño experimental en la siguiente ola de la encuesta, en el cual se asignaba un incentivo monetario adicional a la mitad de esos hogares. Sus resultados mostraron incrementos en las tasas de respuesta de los hogares tratados, y animaron a sus responsables a seguir utilizando este diseño adaptativo en futuras ediciones.


La regla tradicional utilizada para diseñar encuestas ha sido la estandarización de todos los procesos y protocolos. Todas unidades muestrales deben ser tratadas de la misma manera. Con la excepción de las introducciones de los entrevistadores (\cite{groves1992understanding}), esta regla se mantuvo durante bastante tiempo. Pero el aumento de las reticencias de la población a participar en encuestas y los recortes presupuestarios llevó a buscar cómo mejorar la eficiencia de los procesos de elaboración de encuestas, y empezaron a considerarse diseños adaptativos enfocados a subgrupos específicos de la muestra (\cite{groves2006responsive}, \cite{lynn2014targeted}, \cite{lynn2017standardised}). En ese sentido, la predicción se ha convertido en una opción muy interesante para poder identificar anticipadamente a individuos que potencialmente podrían abandonar precipitadamente una encuesta longitudinal, y en los que los métodos de machine learning tienen un peso importante ya que no necesitan un conocimiento previo de las relaciones que se quieren estudiar, y suelen adaptarse bien a contextos en los que la relación entre la variable dependiente y sus predictores suele ser compleja y no lineal (\cite{buskirk2018introduction}, \cite{kern2019tree}, \cite{kern2021predicting}, \cite{jankowsky2022validation}).

Los estudios de modelos predictivos realizan una comparación de rendimiento entre modelos tradicionales utilizados para analizar panel attrition, casi siempre una regresión logística, y modelos basados en métodos de Machine Learning, como diferentes tipos de árboles de decisión o máquinas de soporte de vectores. \cite{kern2019tree} y \cite{kern2021predicting} muestran casos en los que los modelos predictivos basados en machine learning, especialmente árboles de decisión, presentan resultados prometedores. Sin embargo, en \cite{jankowsky2022validation} apenas observan diferencias significativas entre los resultados de un modelo de regresión logística y los de un modelo GBM (Gradient Boosting Machine). La conclusión de ese artículo es que no necesariamente un modelo más complejo (y más complejo de entender) puede ser más adecuado para predecir panel attrition, y por tanto utilizarse para diseñar políticas adaptativas para reducirlo.

La intención de este proyecto es realizar un estudio similar a los propuestos por \cite{kern2021predicting} y \cite{jankowsky2022validation}, y comprobar si modelos basados en métodos de machine learning pueden predecir adecuadamente el panel attrition en la EFF, y dar la posibilidad de contribuir al desarrollo de herramientas que ayuden a reducirlo.
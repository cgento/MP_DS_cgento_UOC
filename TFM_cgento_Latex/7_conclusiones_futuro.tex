\chapter{Conclusiones y líneas de trabajo futuras}
\label{chapter:conclusiones}

El objetivo principal de este Trabajo de Fin de Máster es desarrollar un modelo basado en algoritmos de Machine Learning que ayude a predecir si un hogar panel de la Encuesta Financiera de las Familias (EFF) dejará de participar en la siguiente edición. Para ello, se ha seguido la implementación de entrenamiento y evaluación realizada en \cite{beste2023case}, pero adaptada al caso de estudio de la EFF.

Este trabajo se ha divido en dos piezas de análisis. La primera es un análisis exploratorio de las variables de la EFF y ver su posible relación con la participación de los hogares en la siguiente edición. En la segunda se entrenan cuatro modelos basados en algoritmos de Machine Learning (CART, Random Forest, XGBooster y Naïve-Bayes) para predecir el Panel Attrition en la EFF2020 con datos de la EFF2017, y a continuación se compara y evalúa su rendimiento comparado con el ofrecido por un modelo de Regresión Logística de efectos principales, que es un modelo que se ha utilizado tradicionalmente para analizar el Panel Attrition.

El trabajo de análisis exploratorio ha supuesto un desafío ya que ha requerido manejar, combinar y homogeneizar información de un total de doce ficheros de datos repartidos entre tres ediciones de la EFF. Siete de ellos contenían registros de hogares, otros dos registros de incidencias, otros dos registros de pantallas de interacción con un software y el último era un censo de entrevistadores. Además, dos de los ficheros contenían más de 6,000 variables, y otros dos más de 600. En una misma edición de la encuesta había algunas variables duplicadas en diferentes ficheros, y también algunas variables comunes entre olas tenían diferentes codificaciones dependiendo de la ola. Tras eliminar duplicados, redundancias y homogeneizar variables, se consiguió un conjunto de datos para el entrenamiento de 5,397 hogares y 57 variables predictoras.

El análisis exploratorio ha mostrado varios resultados interesantes que pueden ser útiles para hacer implementaciones de diseños adaptativos. La proporción de hogares panel que no participaron en la EFF2020 era mayor para hogares que en 2017 participaron por primera vez, se mostraron más recelosos, no consintieron grabar la entrevista, la realizaron con un proxy. También lo era en aquellos hogares cuya PR en 2017 tenía un menor nivel de estudios, tenía menos de 35 años o más de 74, mostraron un menor nivel de satisfacción con su vida y un menor interés durante la entrevista. Finalmente, la proporción de abandonos en 2020 también fue mayor para los hogares cuya renta de 2017 se situaba en las tres quintilas inferiores de la distribución de renta de los hogares españoles en 2017, su riqueza bruta en 2017 se encontrada en dos las quintilas inferiores de la distribución de riqueza bruta de todos los hogares españoles en 2017, y si no poseían deudas. Toda esta información puede ser útil para un entrevistador que va a visitar a un hogar panel porque puede adaptar los argumentos que utilice para convencerles. Por ejemplo, si el hogar se mostró receloso en la edición anterior, puede ser más útil enfocarse en hablar sobre la confidencialidad de la encuesta que la presencia del estudio en los medios de comunicación.

En la parte del entrenamiento y evaluación de los modelos de Machine Learning, ninguno de los modelos entrenados llega a superar el rendimiento del modelo de Regresión Logística para la predicción del conjunto de test. Además, el valor de la ROC AUC de este modelo Logit no supera el valor de 0.6, lo cual lo clasifica como un predictor malo. Para intentar aprender sobre los errores de predicción, se han usado esos mismos modelos para hacer la predicción con el conjunto de entrenamiento y hacer una comparación entre los resultados del conjunto de test y los de entrenamiento. Todos los modelos presentan mejores resultados para el conjunto que entrenamiento que para el de test, y además los modelos de Random Forest y XGBooster sí presentan valores más altos de ROC AUC que el modelo Logit. Sin embargo, la mejor métrica, la del Random Forest, no llega a superar el valor de 0,7, que lo clasifica como un predictor regular. No parece que los modelos hagan overfitting.

Finalmente, para aprender cómo ha sido el entrenamiento del Random Forest, se observa la importancia de los veinte predictores con mayor valor de importancia. Destacan que la PR ya formase parte del hogar desde al menos dos ediciones antes, el valor del interés mostrado por la PR, que el entrevistador considerase que el hogar participó por la relevancia de la encuesta y algunas variables analizadas durante la exploración de los datos, en concreto la tenencia de deudas pendientes, el número de olas en las que ha participado el hogar, que la PR consintiera grabar la entrevista y el nivel educativo de la PR.

A partir de los resultados que se han visto en este proyecto, se plantean las siguientes reflexiones y los posibles pasos que se podrían dar en el futuro para este proyecto:

\begin{enumerate}
    \item \textbf{Revisar la selección de variables:} Los resultados del análisis descriptivo muestran que las variables seleccionadas tienen relación con la participación de los hogares, pero da la sensación de que no tienen suficiente poder para predecir la participación de los hogares en las ediciones siguientes. Esta selección se inspiró en las implementaciones de \cite{kern2021predicting} y \cite{beste2023case}, que son ejemplos de buenas predicciones de Panel Attrition en encuestas a hogares, como la EFF. Sin embargo, sus métodos de recolección de datos, frecuencias entre olas y muestreos son diferentes a los de la EFF. En \cite{jankowsky2022validation} mencionan que esos factores pueden afectar a los resultados de los modelos y por tanto no puedan generalizarse a otras encuestas. Utilizar variables similares es un buen punto de partida, pero hay que profundizar más en la búsqueda de variables que puedan adaptarse mejor al caso de la EFF. Afortunadamente en esta encuesta se recogen muchas variables que no se han utilizado en este estudio y su inclusión es viable. Esto abre dos líneas de trabajo futuro continuistas:
    \begin{enumerate}[noitemsep]
        \item Revisar la selección de variables actual, añadir las que se hayan dejado fuera y puedan tener poder predictivo, y eliminar aquellas que puedan estar generando ruido. Por ejemplo, implementar procesos de Feature Selection basados en algoritmos de Machine Learning.
        \item Hacer un análisis más profundo del error de predicción. En concreto, analizar de manera específica aquellos hogares que los modelos predicen mal y ver qué características tienen.
    \end{enumerate}
    \item \textbf{Considerar la viabilidad de incluir variables de la ola actual como predictores:} Como los modelos de predicción se basan en información disponible ex-ante, en los trabajos de investigación de referencia que se han consultado para este proyecto siempre se considera utilizar estrictamente la información de ediciones pasadas de las encuestas para predecir la participación en las ediciones siguientes. Sin embargo, hay información relacionada con la edición en curso que podría ser tanto o más relevante para explicar, y tal vez predecir, el Panel Attrition. Por ejemplo, el papel del entrevistador es muy importante para conseguir la colaboración de los hogares, ya que son ellos los que van a visitarles personalmente e intentan convencerles para volver a participar. En ese sentido, una opción muy interesante sería tomar toda la información disponible de las olas anteriores y la ola en curso (entrevistador, visitas, fechas...), hacer un análisis explicativo para ver cuáles son las variables que tienen relaciones más fuertes con el Panel Attrition, y ver qué variables conocidas de la edición en curso podrían utilizarse de manera viable para hacer una predicción de la participación, y poder utilizar esa información para diseñar implementaciones para aumentar la participación.
    \item \textbf{El posible efecto del Covid-19 en el rendimiento de los modelos:} Las olas de la EFF seleccionadas para el estudio son EFF2017, EFF2020 y EFF2022 porque son las que tienen mayor cantidad de datos y también los de mayor calidad. Sin embargo, como se comentó al final de la sección \ref{section:etapas_eff}, la pandemia del Covid-19 obligó a cambiar la metodología de entrevista de la EFF2020 de presencial a telefónica para poder hacer la encuesta respetando las condiciones de seguridad y distancia social. Todos los modelos entrenados utilizan datos de 2017 para predecir 2020, que es año de Covid. Y, posteriormente, en el test se utiliza información recogida durante ese año 2020 para predecir Panel Attrition en el año 2022, cuando ya se estaba superando la pandemia y fue posible recuperar las entrevistas presenciales. Es muy razonable pensar que tanto la participación de los hogares en la EFF2020 como los datos recopilados en esa edición se vieron afectados por el miedo al Covid. Y, por tanto, que las causas de participación en 2020 sean muy diferentes a las de 2022. Ante esta problemática, se plantean dos posibles alternativas:
    \begin{enumerate}[noitemsep]
        \item \textbf{Hacer un modelo de predicción con los datos de las olas previas a 2020:} La metodología de todas estas olas es homogénea porque todas las entrevistas fueron personales, y no hubo una crisis como la del Covid-19. El inconveniente es que sólo se podría utilizar la información de las respuestas de los hogares. Pero no está claro si necesariamente el rendimiento sería peor.
        \item \textbf{Repetir el actual ejercicio cuando haya más ediciones posteriores a 2020:} La EFF va a continuar realizándose en los próximos años con una frecuencia bienal. Se puede volver a plantear esta misma metodología dentro unos años utilizando sólo ediciones completadas después de 2020, y con el beneficio de recoger toda la información que se recoge actualmente y que no está disponible para antes de 2017.
    \end{enumerate}
    \item\textbf{Cambiar el enfoque:}: Siempre existe la opción de considerar enfoques alternativos sean interesantes y más adecuados para el problema que se quiere abordar. En ese sentido, hay dos alternativas bastante interesantes:
    \begin{enumerate}[noitemsep]
        \item \textbf{Predecir la participación de los paneles en su segunda ola}: En la exploración de los datos se vio que los hogares que han participado sólo en una edición muestran más proporción de abandonos que los que han participado más de dos años. Esto invita a pensar que los hogares que han participado más de una vez están más comprometidos con el estudio, y seguramente merezca la pena enfocar el análisis en la predicción de la participación de los panelistas que sólo han participado una vez en lugar de predecir la participación de todos los hogares.
        \item \textbf{Realizar un análisis de supervivencia:} En vez de predecir un resultado binario, de participar o no participar, se puede plantear hacer un ejercicio de análisis de supervivencia (survival analysis), e intentar predecir el número de ediciones en las que participará un hogar de la EFF antes de abandonar el estudio.
    \end{enumerate}
\end{enumerate}